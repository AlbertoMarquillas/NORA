## xx. Ejecuci√≥n del Proyecto NORA ‚Äì Bit√°cora T√©cnica

Este documento sirve como registro paso a paso del proceso de ejecuci√≥n t√©cnica del proyecto NORA, en orden cronol√≥gico y estructurado. Permite trazar c√≥mo se ha ido implementando y desplegando cada parte del sistema.

Se actualizar√° progresivamente con cada acci√≥n relevante, incluyendo comandos ejecutados, archivos creados, estructura establecida y decisiones t√©cnicas documentadas.

---

### üü¢ Paso 1: Generaci√≥n de estructura de carpetas l√≥gicas (`src/`)

**Fecha:** [22/04/2025]

**Acci√≥n:** Creaci√≥n de la estructura modular del sistema dentro de `src/` mediante script automatizado.

**Ubicaci√≥n del script:** `src/utils/estructura_src.py`

**Comando ejecutado:**
```bash
cd src
python utils/estructura_src.py
```

**Resultado:**
- Carpetas creadas: `vision/`, `voz/`, `interfaz/`, `control/`, `sistema/`, `datos/`
- Cada una incluye su `__init__.py`

**Motivaci√≥n t√©cnica:** Permite comenzar con un dise√±o desacoplado y extensible. Cada carpeta corresponde a un m√≥dulo l√≥gico aut√≥nomo del sistema.

**Notas adicionales:**
- El script ha sido corregido para **no crear la carpeta `src/`**, ya que se ejecuta desde dentro de ella.
- Ver `utils/estructura_src.py` para la versi√≥n actual.

**Referencias:**
- `02.arquitectura_logica.md`
- `estructura_src.py`

---

### üü¢ Paso 2: Reestructuraci√≥n del software bajo carpeta dedicada `software/`

**Fecha:** [22/04/2025]

**Acci√≥n:** Reubicaci√≥n de la l√≥gica principal dentro de una nueva carpeta `software/`, incluyendo `main.py` y la carpeta `src/` como subdirectorio.

**Estructura resultante:**
```bash
software/
‚îú‚îÄ‚îÄ main.py
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ vision/
    ‚îú‚îÄ‚îÄ voz/
    ‚îú‚îÄ‚îÄ interfaz/
    ‚îú‚îÄ‚îÄ control/
    ‚îú‚îÄ‚îÄ sistema/
    ‚îî‚îÄ‚îÄ datos/
```

**Detalles:**
- Se ha creado `software/main.py` como punto de entrada del sistema.
- Se ha a√±adido `software/src/__init__.py` para declarar `src/` como paquete Python.
- Los imports dentro de `main.py` podr√°n usar `from src.<> import ...`

**Motivaci√≥n t√©cnica:** Agrupar todo el software operativo en una √∫nica carpeta para aislarlo de documentaci√≥n, hardware y otros recursos. Mejora la portabilidad y claridad de prop√≥sito.

**Referencias:**
- `README.md` actualizado con nueva estructura
- `main.py` inicializado como lanzador del sistema en modo simulaci√≥n

---

### üü¢ Paso 3: Creaci√≥n del m√≥dulo `sistema/fsm.py` con la FSM de NORA

**Fecha:** [22/04/2025]

**Acci√≥n:** Se define el archivo `fsm.py` dentro del m√≥dulo `sistema/` para gestionar los estados funcionales de NORA mediante una m√°quina de estados finita (FSM).

**Contenido principal:**
- Enumeraci√≥n `EstadoNora` con los estados: `REPOSO`, `PASIVO`, `ESCUCHA`, `ACTIVO`, `DESPEDIDA`, `ERROR`.
- Clase `FSM` con atributo `estado_actual` y m√©todo `transicion(evento)` para gestionar las reglas de paso entre estados seg√∫n eventos del sistema.

**Motivaci√≥n t√©cnica:** La FSM es el n√∫cleo l√≥gico que define el comportamiento global del asistente. Determina c√≥mo debe reaccionar a cada evento y qu√© m√≥dulo debe activarse en cada estado.

**Referencias:**
- `04.estados_y_emociones.md` (para coherencia funcional)
- `06.documento_eventos.md` (para mapeo de eventos v√°lidos)
- Archivo: `software/src/sistema/fsm.py`

---

### üü¢ Paso 4: Integraci√≥n de la FSM en `main.py` con eventos simulados

**Fecha:** [22/04/2025]

**Acci√≥n:** Se modifica `main.py` para importar la clase `FSM` y ejecutar una simulaci√≥n de eventos, verificando las transiciones de estado.

**Eventos simulados:**
- `EVT_NFC_ACTIVATE`
- `EVT_FACE_DETECTED`
- `EVT_COMMAND_RECOGNIZED`
- `EVT_IDLE_TIMEOUT`
- `EVT_SHUTDOWN_REQUEST`

**Resultado observado:**
```plaintext
[main.py] Estado inicial: REPOSO
‚Üí Enviando evento: EVT_NFC_ACTIVATE ‚Üí PASIVO
‚Üí Enviando evento: EVT_FACE_DETECTED ‚Üí ESCUCHA
‚Üí Enviando evento: EVT_COMMAND_RECOGNIZED ‚Üí ACTIVO
‚Üí Enviando evento: EVT_IDLE_TIMEOUT ‚Üí PASIVO
‚Üí Enviando evento: EVT_SHUTDOWN_REQUEST ‚Üí PASIVO (sin efecto)
```

**Motivaci√≥n t√©cnica:** Verificar el flujo de transici√≥n y preparar la base para conectar el sistema de eventos reales.

**Referencias:**
- `main.py` actualizado en `software/`
- Salida de consola del entorno virtual (`python main.py --simulacion`)

---

### üü¢ Paso 5: Creaci√≥n del `EventManager` para gesti√≥n de eventos internos

**Fecha:** [22/04/2025]

**Acci√≥n:** Se implementa el archivo `event_manager.py` en el m√≥dulo `sistema/`, con las clases `EventManager` y `Evento`.

**Contenido principal:**
- `Evento`: estructura de evento con tipo, origen, datos, timestamp y prioridad
- `EventManager`: subscripci√≥n y distribuci√≥n de eventos a m√≥dulos receptores

**Caracter√≠sticas t√©cnicas:**
- Uso de `PriorityQueue` para orden por prioridad (1 a 5)
- Distribuci√≥n asincr√≥nica y desacoplada v√≠a callbacks
- Registro de subscripciones por tipo de evento
- Consola informativa de encolado y despacho de eventos

**Motivaci√≥n t√©cnica:** Establecer el sistema de mensajer√≠a interna para garantizar modularidad y permitir que los distintos componentes interact√∫en sin acoplamiento directo.

**Referencias:**
- `06.documento_eventos.md`
- Archivo: `software/src/sistema/event_manager.py`

---

### üü¢ Paso 6: Integraci√≥n de FSM con EventManager en `main.py`

**Fecha:** [22/04/2025]

**Acci√≥n:** Se reemplaza la l√≥gica de simulaci√≥n directa en `main.py` por una arquitectura basada en `EventManager`, suscribiendo la FSM como receptora de eventos.

**Cambios clave:**
- Se instancia `EventManager` en `main.py`
- Se registra `FSM.transicion()` como callback de m√∫ltiples eventos (`EVT_NFC_ACTIVATE`, `EVT_FACE_DETECTED`, etc.)
- Los eventos simulados se emiten y se procesan con el EventManager en lugar de llamar directamente a la FSM

**Resultado:**
- El sistema responde a eventos mediante el canal de distribuci√≥n previsto
- La arquitectura se vuelve desacoplada y lista para integraci√≥n progresiva de m√≥dulos

**Referencias:**
- `main.py` actualizado (software/)
- `event_manager.py` limpio sin bloque de pruebas
- `test_event_manager_fsm.py` en `software_tests/` para pruebas externas

---

### üü¢ Paso 7: Creaci√≥n del m√≥dulo `voz/reconocedor.py` para simulaci√≥n de voz

**Fecha:** [22/04/2025]

**Acci√≥n:** Se implementa el archivo `reconocedor.py` dentro del m√≥dulo `voz/`, con una clase `ReconocedorVoz` encargada de simular el reconocimiento de voz.

**Funcionamiento:**
- Simula un comando de voz con probabilidad de √©xito (70%).
- Emite evento `EVT_COMMAND_RECOGNIZED` con el texto si el reconocimiento tiene √©xito.
- Emite `EVT_COMMAND_UNKNOWN` en caso contrario.
- Todos los eventos se emiten mediante el `EventManager`, integr√°ndose con la FSM.

**Motivaci√≥n t√©cnica:** Permite probar el sistema de interacci√≥n por voz de forma modular, controlada y sin requerir micr√≥fono ni ASR real, en preparaci√≥n para su sustituci√≥n futura por Vosk o Whisper.

**Observaci√≥n:** Este m√≥dulo ha sido dise√±ado para ser f√°cilmente reemplazado por reconocimiento real, y podr√° escalar hacia IA conversacional conectando la transcripci√≥n a un modelo LLM externo o interno.

**Referencias:**
- Archivo: `software/src/voz/reconocedor.py`
- Documento: `01.plan_implementacion_software.md`, secci√≥n "Observaciones t√©cnicas futuras"

---

### üü¢ Paso 8: Integraci√≥n del Reconocedor de Voz en `main.py`

**Fecha:** [22/04/2025]

**Acci√≥n:** Se ha integrado el m√≥dulo `ReconocedorVoz` dentro de `main.py`, permitiendo que el sistema simule el reconocimiento de voz como parte del flujo principal.

**Cambios realizados:**
- Se importa la clase `ReconocedorVoz` desde `src.voz.reconocedor`
- Se instancia un objeto `voz = ReconocedorVoz(em)` conectado al `EventManager`
- Tras eventos clave (`EVT_FACE_DETECTED` y `EVT_COMMAND_RECOGNIZED`), se activa `voz.escuchar_simulado()`
- El resultado del reconocimiento simulado genera un evento `EVT_COMMAND_RECOGNIZED` o `EVT_COMMAND_UNKNOWN`

**Motivaci√≥n t√©cnica:** Integrar de forma natural el m√≥dulo de voz como un productor activo de eventos dentro del sistema. Permite simular flujos completos de interacci√≥n sin hardware ni entrada real de voz, y probar la reacci√≥n de la FSM.

**Resultado esperado:**
- Transici√≥n FSM ‚Üí ESCUCHA
- Reconocimiento simulado (con 70% √©xito)
- Evento emitido por voz ‚Üí FSM cambia estado seg√∫n l√≥gica definida

**Referencias:**
- Archivo: `software/main.py`
- Archivo: `software/src/voz/reconocedor.py`
- Documento: `01.plan_implementacion_software.md`

---


### üü¢ Paso 9: Dise√±o del m√≥dulo de s√≠ntesis de voz (TTS)

**Fecha:** [22/04/2025]

**Acci√≥n:** Se define la necesidad funcional y la estructura inicial del m√≥dulo `voz/sintetizador.py`, responsable de convertir texto en voz para emitir respuestas habladas por parte de NORA.

**Planteamiento t√©cnico:**
- Se utilizar√° `pyttsx3` como motor TTS inicial por ser offline, multiplataforma y ligero.
- El m√≥dulo se integrar√° con el `EventManager` mediante suscripci√≥n al evento `EVT_DECIR_TEXTO`.
- Cada evento contendr√° un campo `datos['texto']` con el contenido a sintetizar.

**Motivaci√≥n t√©cnica:** Proveer a NORA de capacidad de respuesta verbal coordinada, manteniendo el enfoque desacoplado mediante eventos. Este m√≥dulo complementa el flujo iniciado por `ReconocedorVoz`, cerrando el ciclo de entrada-salida verbal.

**Flujo previsto:**
1. Usuario activa escucha ‚Üí Reconocedor genera `EVT_COMMAND_RECOGNIZED`
2. FSM u otro m√≥dulo decide respuesta ‚Üí emite `EVT_DECIR_TEXTO`
3. Sintetizador recibe evento ‚Üí habla el texto proporcionado

**Referencias:**
- Futuro archivo: `software/src/voz/sintetizador.py`
- Motor propuesto: `pyttsx3`
- Formato de evento: `Evento(tipo="EVT_DECIR_TEXTO", datos={"texto": "Son las 4 en punto"})`

---

### üü¢ Paso 10: Integraci√≥n del m√≥dulo de s√≠ntesis de voz (TTS) en `main.py`

**Fecha:** [22/04/2025]

**Acci√≥n:** Se ha integrado el m√≥dulo `SintetizadorVoz` al flujo principal del sistema, utilizando `pyttsx3` para emitir respuestas habladas al recibir eventos `EVT_DECIR_TEXTO`.

**Cambios realizados:**
- Creaci√≥n del archivo `voz/sintetizador.py` con clase `SintetizadorVoz`
- Registro en `main.py` para que escuche eventos `EVT_DECIR_TEXTO`
- El `main.py` emite una respuesta simulada al detectar un comando de voz v√°lido (`EVT_COMMAND_RECOGNIZED`)

**Motivaci√≥n t√©cnica:** Completar el ciclo de interacci√≥n verbal: reconocimiento simulado + respuesta hablada. Este paso permite validar el subsistema de salida por voz y prepara el sistema para coordinaci√≥n con expresiones faciales y emocionales en el futuro.

**Resultado esperado:**
- Evento de voz simulado ‚Üí FSM responde ‚Üí emite texto a decir ‚Üí `SintetizadorVoz` habla la respuesta con `pyttsx3`

**Referencias:**
- Archivo: `software/src/voz/sintetizador.py`
- Archivo: `software/main.py`
- Evento gestionado: `EVT_DECIR_TEXTO`

---

### üü¢ Paso 11: Modularizaci√≥n de `main.py` con manejadores externos

**Fecha:** [22/04/2025]

**Acci√≥n:** Se ha refactorizado `main.py` para extraer la l√≥gica de manejo de eventos FSM a un m√≥dulo externo, con el objetivo de mejorar la modularidad, limpieza y reutilizaci√≥n del c√≥digo.

**Cambios realizados:**
- Creaci√≥n del archivo `sistema/manejadores.py` con la funci√≥n `manejar_evento_fsm()`
- Uso de `functools.partial()` en `main.py` para registrar `manejar_evento_fsm()` como manejador de eventos, manteniendo acceso a `fsm` y `em`
- Limpieza del cuerpo de `iniciar_sistema()` eliminando funciones internas

**Motivaci√≥n t√©cnica:** Facilitar la escalabilidad y mantenimiento del sistema al separar responsabilidades y encapsular manejadores por m√≥dulo. Esta pr√°ctica permite centralizar todos los manejadores futuros en un √∫nico archivo.

**Resultado esperado:**
- L√≥gica de transici√≥n FSM + emisi√≥n de respuestas movida fuera del `main.py`
- Suscripci√≥n clara y declarativa de eventos en el bloque de inicializaci√≥n

**Referencias:**
- Archivo: `software/main.py`
- Archivo: `software/src/sistema/manejadores.py`

---

### üü¢ Paso 12: Dise√±o del m√≥dulo `interfaz/` para representaci√≥n simb√≥lica

**Fecha:** [22/04/2025]

**Acci√≥n:** Se planifica la creaci√≥n del m√≥dulo `interfaz/` con una clase `InterfazSimulada`, cuyo prop√≥sito ser√° representar gr√°ficamente (de forma textual o visual) los estados y emociones del asistente NORA.

**Motivaci√≥n t√©cnica:** Complementar la interacci√≥n verbal con retroalimentaci√≥n visual simb√≥lica, incluso en modo de simulaci√≥n. Este m√≥dulo podr√° mostrar expresiones, colores simb√≥licos o mensajes que acompa√±en el comportamiento del sistema.

**Objetivos principales del m√≥dulo:**
- Mostrar visualmente el estado actual de NORA (REPOSO, ESCUCHA, ACTIVO, ERROR, etc.)
- Representar emociones b√°sicas (neutro, alegre, confundido, dormido)
- Responder a eventos tipo `EVT_COMMAND_RECOGNIZED`, `EVT_COMMAND_UNKNOWN`, `EVT_FACE_DETECTED`, `EVT_IDLE_TIMEOUT`, etc.
- Mantener una l√≥gica desacoplada, escuchando eventos desde el `EventManager`

**Representaci√≥n prevista (modo texto):**
- üòê Neutro (estado PASIVO)
- üü¢ Activo (tras comando entendido)
- ‚ùì Confuso (tras comando no reconocido)
- üí§ Inactivo (tras timeout o reposo)

**Referencias:**
- Futuro archivo: `software/src/interfaz/interfaz.py`
- EventManager ya preparado para distribuir eventos

---

### üü¢ Paso 13: Integraci√≥n indirecta de TTS e InterfazSimulada v√≠a EventManager

**Fecha:** [Especificar]

**Acci√≥n:** Se confirma la integraci√≥n completa de los m√≥dulos `SintetizadorVoz` y `InterfazSimulada` dentro de `main.py`, aunque no son invocados directamente. Ambos act√∫an como m√≥dulos pasivos, suscritos al `EventManager`, reaccionando a eventos espec√≠ficos.

**Detalles t√©cnicos:**
- `SintetizadorVoz` escucha `EVT_DECIR_TEXTO` y convierte el contenido en voz con `pyttsx3`.
- `InterfazSimulada` escucha m√∫ltiples eventos (`EVT_COMMAND_RECOGNIZED`, `UNKNOWN`, etc.) y muestra representaciones simb√≥licas (emojis + mensajes) por consola.
- Ambos m√≥dulos **no requieren llamadas expl√≠citas en `main.py`**, sino que act√∫an cuando reciben sus eventos registrados.

**Motivaci√≥n t√©cnica:** Confirmar y documentar el dise√±o desacoplado basado en eventos. Esta integraci√≥n demuestra que el sistema es modular, extensible y que cada componente responde solo a los est√≠mulos que le corresponden.

**Resultado esperado:**
- El sistema habla al reconocer comandos v√°lidos.
- Informa visualmente en consola al detectar presencia, reconocer o no comandos, o entrar en reposo.

**Referencias:**
- Archivos: `software/src/voz/sintetizador.py`, `software/src/interfaz/interfaz.py`
- Integraci√≥n comprobada en `main.py` mediante instanciaci√≥n indirecta

---

### üîú Pr√≥ximos pasos previstos

1. A√±adir visualizaci√≥n basada en estado actual (FSM ‚Üí interfaz)
2. Emitir eventos emocionales simb√≥licos (`EVT_EMOCION_ALEGRE`, etc.)
3. Desacoplar `main.py` en una clase `Sistema` para escalabilidad y pruebas unitarias

Este archivo se actualizar√° de forma incremental conforme se ejecuten nuevas acciones en el entorno local del proyecto.