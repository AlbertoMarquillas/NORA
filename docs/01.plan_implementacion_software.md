## Plan de Implementación Inicial – Proyecto NORA (Software First)

### Enfoque estratégico

Dado que NORA es un sistema físico con arquitectura modular, se ha decidido iniciar el desarrollo por la capa de **software**, permitiendo simular funcionalidades clave sin necesidad de tener el hardware construido. Este enfoque posibilita:

- Validar los flujos de interacción multimodal desde el principio.
- Diseñar y depurar la arquitectura lógica de forma aislada.
- Establecer una base funcional reutilizable para la integración física posterior.
- Implementar pruebas automatizadas por módulo desde etapas tempranas.

La metodología adoptada sigue un modelo de desarrollo **iterativo por fases**, con pruebas constantes y documentación técnica paralela.

---

### Fases de implementación (software)

#### Fase 1: Núcleo funcional mínimo (MVP lógico)

**Objetivo:** establecer un prototipo funcional que simule la lógica de interacción principal de NORA sin dependencia del hardware real.

**Componentes principales:**
- **Reconocimiento de voz offline (ASR):** utilización de Vosk o Whisper en local para convertir audio a texto. Se configura un vocabulario básico de interacción.
- **FSM de diálogo:** lógica de control conversacional basada en máquina de estados finita, diseñada para manejar intenciones básicas.
- **Síntesis de voz (TTS):** implementación inicial con `pyttsx3`, adaptable a otros motores. Permite entonación ajustable por contenido.
- **Simulación visual facial:** uso de una ventana gráfica (Tkinter o PyGame) para representar un rostro animado con expresiones básicas (ojos, boca, cejas).
- **Estados internos:** implementación de los modos *reposo*, *escucha*, *activo*, con transiciones visuales y de comportamiento.
- **Base de datos local (SQLite):** almacenamiento estructurado de notas, eventos y comandos del usuario. Interfaz simple para consulta por voz.
- **Simulación de entrada visual:** detección de presencia y rostro mediante webcam usando OpenCV; sin necesidad aún de servos.

#### Fase 2: Integración de visión y expresividad

**Objetivo:** enriquecer la interacción con percepción visual activa y capacidad expresiva coherente.

**Tareas previstas:**
- **Detección facial avanzada:** uso de MediaPipe para seguimiento de rostro y estimación de orientación.
- **Estimación postural:** integración de modelo pose estimation para identificar posiciones mantenidas.
- **Módulo de atención visual:** lógica que activa estados o respuestas al detectar mirada o presencia mantenida.
- **Animaciones expresivas:** generación dinámica de emociones visuales en pantalla mediante cambios en rostro digital.
- **Iluminación RGB virtual:** representación gráfica de iluminación adaptativa que indica estados o emociones.

#### Fase 3: Control físico virtualizado

**Objetivo:** abstraer el control de hardware para preparar la migración futura a la Raspberry Pi sin modificar lógica central.

**Desarrollos clave:**
- **Interfaces GPIO simuladas:** diseño de clases `MockGPIO`, `MockPWM` que emulan comportamiento de pines reales.
- **Motor de servos abstracto:** simulación visual o textual del movimiento físico de cabeza (inclinaciones, orientación).
- **Activación NFC virtual:** evento gatillado por teclado o GUI para simular lectura NFC, enlazado a perfil de usuario.
- **Gestión modular de estados:** control independiente y desactivación selectiva de módulos (visión, voz, datos).

#### Fase 4: Integración general y sincronización

**Objetivo:** consolidar todos los módulos en una arquitectura coherente y expandible.

**Acciones clave:**
- **Bus interno de eventos:** implementación de `EventManager` o sistema de señales tipo `pub/sub`.
- **Priorización de estímulos:** lógica que regula conflictos entre entradas visuales, verbales o temporales.
- **Modelo de emociones simuladas:** motor de transición emocional basado en contexto y eventos (alegría, duda, espera).
- **Coordinación multimodal:** cada acción se ejecuta con voz, expresión facial e iluminación sincronizada.
- **Framework de pruebas:** scripts en `/tests/` para verificar la respuesta integrada del sistema.

---

### Herramientas y entorno de desarrollo

- **Lenguaje principal:** Python 3.10+
- **Visión artificial:** OpenCV + MediaPipe (detección facial, postural, de presencia)
- **Reconocimiento de voz (ASR):** Vosk o Whisper en local, con gramática adaptativa
- **Síntesis de voz (TTS):** pyttsx3 como opción inicial (sin conexión a internet)
- **Interfaz visual:** Tkinter, PyGame o PyQt para simulación del rostro e iluminación
- **Base de datos:** SQLite3 con ORM simple o acceso directo
- **Mock GPIO:** clases personalizadas o librerías de emulación como `gpiozero.mock` para Raspberry Pi
- **Gestión de eventos:** `queue`, `asyncio` o sistema propio basado en señales
- **Control de versiones:** Git, con estructura de ramas por módulo funcional

---

### Beneficios del enfoque software-first

- Permite avanzar en diseño e interacción sin retraso por construcción física.
- Mejora la mantenibilidad mediante pruebas tempranas y modularidad lógica.
- Facilita la creación de documentación técnica, esquemas y diagramas funcionales por módulo.
- Asegura una integración ordenada del hardware una vez validada la lógica base.
- Habilita el uso del sistema en entornos simulados, demostraciones o formación sin montaje completo.