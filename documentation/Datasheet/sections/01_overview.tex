\section*{1. General Overview}
\addcontentsline{toc}{section}{1. General Overview}

NORA is a modular physical assistant designed to operate autonomously with embedded voice, vision, and behavioral intelligence. It serves as a platform for interactive applications in personal, educational, and experimental environments.

\vspace{0.5cm}

The system combines multiple subsystems in a unified architecture:

\begin{itemize}
	\item \textbf{Multimodal Interaction:} Speech recognition, synthesis, and computer vision for human-centered communication.
	\item \textbf{Modular Control:} Finite State Machine (FSM)-based control logic, agent-based decision modules, and customizable behaviors.
	\item \textbf{Physical Mobility:} Support for autonomous movement, obstacle detection, and environmental interaction.
	\item \textbf{Data Awareness:} Integration of a local NAS system for file storage and memory logging.
	\item \textbf{Expansion Ready:} Compatible with EdgeLink protocol for automatic discovery and management of external IoT sensors.
\end{itemize}

\vspace{0.5cm}

Its design philosophy prioritizes:
\begin{itemize}
	\item \textbf{Modularity:} Each component (voice, vision, mobility, storage, agents) can be independently developed, tested, or replaced.
	\item \textbf{Privacy:} All processing is performed locally, without cloud dependency.
	\item \textbf{Transparency:} Fully documented and licensed under MIT for educational and research reuse.
\end{itemize}

\vspace{0.5cm}

NORA can serve as a testbed for embedded AI, robotics, and system integration, offering both real-time interaction and system-level autonomy.


