\section*{1. General Overview}

The NORA system is a modular, intelligent physical assistant designed for embedded operation in personal, educational, and experimental environments. It integrates voice recognition, computer vision, agent-based behavior logic, and physical mobility, all supported by a fully local architecture that eliminates reliance on cloud infrastructure.

The device is built around a decentralized control model, where software modules operate semi-independently and communicate through a central Finite State Machine (FSM) controller. This allows for predictable behavior patterns, easy debugging, and a scalable logic core. NORA operates in real-time, responding to vocal commands, visual stimuli, and environmental events through FSM-driven transitions and adaptive agents.

In its standard configuration, NORA includes a primary processor board (e.g., Raspberry Pi 4 Model B), a camera module, a microphone array, actuators (wheels or servos), and a local storage interface functioning as a NAS. Additional modules, such as an emotion engine, external sensor interfaces, and network-discoverable subsystems, can be enabled depending on the application.

NORA is fully customizable, and each subsystem—voice, vision, motion, memory, and logic—can be independently developed, replaced, or expanded. This makes it suitable for research projects, robotics curricula, personal productivity, or interactive installations where user feedback and autonomy are required.

The system is developed with a privacy-first philosophy. All inference, classification, memory storage, and behavior logic occur within the device boundary. No data is transmitted externally unless explicitly configured.

Key design principles include:
\begin{itemize}
    \item \textbf{Modularity:} Each hardware and software block is isolated, versioned, and extensible.
    \item \textbf{Determinism:} The FSM core ensures traceable behavior and reproducible state transitions.
    \item \textbf{Local Execution:} No cloud dependency is required for primary functionality.
    \item \textbf{Voice and Visual Fusion:} Commands and recognition pipelines can be cross-validated.
    \item \textbf{File-Centric Memory:} Data, logs, and recorded events are stored in a structured, user-accessible format.
\end{itemize}

Multiple modes of interaction are supported, including:

\begin{itemize}
    \item Voice-only operation for environments where screens are not desirable.
    \item Web-based GUI control via a local dashboard, including sensor display and file access.
    \item Autonomous mode with no user input, responding to ambient stimuli or recurring routines.
\end{itemize}

The current firmware version supports real-time speech processing, audio feedback, gesture recognition, movement planning, and file-based recall. Future releases will include integration with reinforcement-based learning agents, improved navigation modules, and wireless sensor auto-discovery (EdgeLink protocol).

NORA is distributed under the MIT License and is suitable for academic, prototyping, or personal enhancement purposes.

